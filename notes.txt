### Decorators

Pada python, decorators berfungsi sebagai pembungkus (wrapper) fungsi supaya bisa menambahkan perilaku baru tanpa mengubah kode aslinya

def my_decorator(func):
    def wrapper():
        print("Sebelum fungsi dijalankan")
        func()
        print("Sesudah fungsi dijalankan")
    return wrapper

@my_decorator
def say_hello():
    print("Hello!")

say_hello()

Berdasarkan kode diatas, function say_hello digunakan sebagai parameter dari function my_decorator, maka func say_hello dijalankan pada saat print pertama selesai dieksekusi.

# Simpan model training

import joblib

joblib.dump(model, "model.pkl")
joblib.dump(vectorizer, "vectorizer.pkl")

# Load model training
model = joblib.load("model.pkl")
vectorizer = joblib.load("vectorizer.pkl")

# TF-IDF
Dikarenakan sebuah komputer tidak bisa memahami teks, kita perlu mengubah teks menjadi angka agar dapat dipahami oleh komputer.

Term Frequency (TF)
TF adalah jumlah kemunculan suatu kata dalam dokumen dibagi dengan jumlah total kata dalam dokumen tersebut.

Inverse Document Frequency (IDF)
IDF adalah logaritma dari jumlah dokumen dibagi dengan jumlah dokumen yang mengandung kata tersebut (mengukur keunikan sebuah kata, semakin besar IDF = semakin unik kata tersebut).

TF-IDF
TF-IDF adalah perkalian antara TF dan IDF.

Di dalam TFidfVectorizer function, terdapat sebuah parameter max_features yang berfungsi untuk memangkas jumlah kata dengan mengambil dari yang terbaik

max_features memilih fitur dengan skor statistik tertinggi berdasarkan frekuensi dan bobot TF-IDF, lalu membuang sisanya.

Aturan praktis (Rule of Thumbs) : 
< 1000          = 1000 - 3000 max features
1000 - 5000     = 3000 - 5000 max features
5.000 – 20.000  = 5.000 – 10.000
> 20.000        = 10.000 – 30.000

5000 = sweet spot untuk sentimen indoensia

# Naive Bayes 
Naive Bayes adalah algoritma klasifikasi berbasis probabilitas yang menggunakan Teorema Bayes.

Terdapat beberapa macam variant dari Naive Bayes, antara lain : 
1. Gaussian NB -> Untuk data numerik kontinue seperti tinggi badan dan suhu
2. Multinomial NB -> Data hitungan / frekuensi seperti text dan TF-IDF
3. Bernoulli NB -> Data biner (0/1) seperti kata ada / tidak
4. Complement NB -> Teks imbalanced seperti spam detection

Dalam analisis sentimen, Multinomial NB cocok digunakan karena mempertimbangkan berapa kali kata muncul sehingga TF-IDF bisa terdistribusi normal. Hal ini dilakukan dengan mengimplementasikan TF-IDF sebagai input dan Naive Bayes sebagai modelnya

Dimensi = jumlah fitur / kolom vektor

# Cara menguji model tidak underfit / overfit

Underfitting = Model terlalu sederhana, gagal menangkap pola penting.
Kamu disuruh menilai emosi dari kalimat, tapi kosakata yang kamu tahu cuma sedikit.

Overfitting = Model terlalu kompleks, hafal data training tapi gagal generalisasi.
Kamu hafal semua contoh latihan, tapi bingung saat ketemu kalimat baru.

- Underfitting = Accuracy data training & testing rendah 
- Overfitting = Accuracy tinggi pada data training dan Akurasi turun di testing
- Good Fit = Training accuracy tinggi & Testing accuracy hampir sama

# InSet (Indonesian Sentimen Lexicon)

Tahapan : 
1. Teks
2. Preprocessing
3. Pencocokan dengan lexicon (penentuan score)
4. Hitung score sentimen 
5. Tentukan Label (score > threshold?)

# Classification Report

FP (False Positive) = diprediksi X, tapi sebenarnya bukan X

contoh : 
Model memprediksi positif = 100 data
Ternyata yang benar-benar positif cuma 85

- Precision = Dari semua data yang diprediksi sebagai kelas X, berapa yang benar-benar X?
- Recall = Dari semua data yang sebenarnya kelas X, berapa yang berhasil ditangkap model?
- F1-Score = Rata-rata harmonis antara precision dan recall
- Support = Jumlah data asli untuk tiap kelas di data uji